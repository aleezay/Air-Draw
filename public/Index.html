<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>AirCanvas Controls — Camera</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html,body{margin:0;background:#0b0b0f;color:#eaeaf0;font:14px/1.45 system-ui}
    #hud{position:fixed;left:12px;top:12px;background:#101018;opacity:.95;padding:10px 12px;border-radius:10px;box-shadow:0 6px 20px rgba(0,0,0,.35)}
    #hud b{font-weight:600}
    video{display:block;width:min(92vw,820px);margin:70px auto 24px;border-radius:12px;box-shadow:0 10px 30px rgba(0,0,0,.35)}
    .small{opacity:.75;font-size:12px}
  </style>
  <script src="/socket.io/socket.io.js"></script>
</head>
<body>
  <div id="hud">
    <div><b>AirCanvas Controls</b></div>
    <div>Calibrated: <span id="cal">no</span></div>
    <div>Gesture: <span id="g">—</span></div>
    <div class="small">Privacy: video stays local</div>
  </div>
  <video id="cam" autoplay playsinline muted></video>

  <script type="module">
    // MediaPipe Tasks Vision — Hand Landmarker (no TensorFlow.js)
    import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

    // --- Signaling (socket.io) + WebRTC DataChannel setup ---
    const signaling = io("/", { transports: ["websocket"] });
    const pc = new RTCPeerConnection();
    const dc = pc.createDataChannel("gestures", { ordered: true });
    dc.onopen = () => console.log("[RTC] datachannel open");

    pc.onicecandidate = e => e.candidate && signaling.emit("signal", { candidate: e.candidate });
    signaling.on("signal", async ({ sdp, type, candidate }) => {
      if (candidate) return pc.addIceCandidate(candidate);
      if (sdp) {
        await pc.setRemoteDescription({ type, sdp });
        if (pc.signalingState !== "stable") {
          const ans = await pc.createAnswer();
          await pc.setLocalDescription(ans);
          signaling.emit("signal", { sdp: ans.sdp, type: ans.type });
        }
      }
    });

    (async () => {
      const offer = await pc.createOffer({ offerToReceiveAudio: false, offerToReceiveVideo: false });
      await pc.setLocalDescription(offer);
      signaling.emit("signal", { sdp: offer.sdp, type: offer.type });
    })();

    // --- Camera setup ---
    const video = document.getElementById('cam');
    const hudG = document.getElementById('g');
    const hudC = document.getElementById('cal');

    const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });
    video.srcObject = stream;
    await new Promise(r => video.onloadedmetadata = r);

    // --- Hand Landmarker ---
    const fileset = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
    );
    const landmarker = await HandLandmarker.createFromOptions(fileset, {
      baseOptions: {
        modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      },
      numHands: 2,
      runningMode: "VIDEO"
    });

    // --- Gesture helpers ---
    let calibrated = false, refSpan = 200;
    let ema = 0, alpha = 0.2; // smoothing

    const norm = (v, a, b) => Math.max(0, Math.min(1, (v - a) / (b - a)));
    const pinchDist = (hand) => { const a = hand[4], b = hand[8]; return Math.hypot(a.x-b.x, a.y-b.y); };
    const orbitAngle = (hand) => { const c = hand[0], p = hand[8]; return Math.atan2(p.y - c.y, p.x - c.x); };

    async function tick() {
      const w = video.videoWidth, h = video.videoHeight;
      const out = await landmarker.detectForVideo(video, performance.now());
      const hands = (out.landmarks || []).map(ls => ls.map(p => ({ x: p.x * w, y: p.y * h })));

      if (!calibrated && hands.length === 2) {
        refSpan = Math.hypot(hands[0][0].x - hands[1][0].x, hands[0][0].y - hands[1][0].y);
        calibrated = true; hudC.textContent = 'yes';
      }

      // Snap-Scale — two-hand span vs reference
      if (calibrated && hands.length === 2) {
        const span = Math.hypot(hands[0][0].x - hands[1][0].x, hands[0][0].y - hands[1][0].y);
        const raw = norm(span / refSpan, 0.6, 1.6);
        ema = ema + alpha * (raw - ema);
        const snapped = Math.round(ema * 20) / 20;
        if (dc.readyState === 'open') dc.send(JSON.stringify({ type: 'scaleSnap', value: snapped }));
        hudG.textContent = `scaleSnap ${snapped.toFixed(2)}`;
      }

      // Orbit — pinch and rotate around palm
      const one = hands[0];
      if (one && pinchDist(one) < 25) {
        const ang = orbitAngle(one);
        if (dc.readyState === 'open') dc.send(JSON.stringify({ type: 'orbit', angle: ang }));
        hudG.textContent = `orbit ${ang.toFixed(2)}`;
      }

      requestAnimationFrame(tick);
    }
    tick();
  </script>
</body>
</html>
